1. 如何解决梯度消失和梯度爆炸的问题？

|问题类型|问题定义|解决措施|
|:--------:|--------|--------|
|梯度消失|根据链式法则，如果`每一层神经元对上一层的输出的偏导乘上权重的结果`都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0.|使用`Relu`和`batch normalization`以及循环神经网络里面的`LSTM`和`GRU`都可以解决这个问题|
|梯度爆炸|根据链式法则，如果`每一层神经元对上一层的输出的偏导乘上权重结果`大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大。|可以使用`梯度截断`、`激活函数`、`Batch Normalization`来解决。|

2. 深度学习调参经验
 - [x] 参数初始化
 - [x] 数据的预处理方式
 - [x] 训练技巧
 - [x] 尽量对数据进行shuffle和augmentation
 - [x] Ensemble
 - [x] 学习率
 - [x] dropout
 - [x] relu+bn
 - [x] 网络层数
 - [x] batch_size
 
3. 逻辑斯蒂回归为什么要对特征进行离散化处理？
  - [x] 离散特征的增加和减少都很容易，易于模型的快速迭代；
  - [x] 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
  - [x] 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
  - [x] 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
  - [x] 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
  - [x] 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
  - [x] 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
  
4. Xgboost是如何处理高维数据的？
